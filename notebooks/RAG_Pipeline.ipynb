{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45de393",
   "metadata": {},
   "source": [
    "# RAG Pipeline\n",
    "\n",
    "## Structure:\n",
    "\n",
    " 1. Extracting text from a .pdf document\n",
    " 2. Chunking \n",
    " 3. Embeddings\n",
    " 4. Vector DB\n",
    " 5. RAG Query \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the path to the PDF file you want to load\n",
    "path = \"../data/raw/AAPL.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94a1a0",
   "metadata": {},
   "source": [
    "### 1. Extracting text from a .pdf document\n",
    "\n",
    "For this we will use the PDFMinerLoader and UnstructuredPDFLoader.\n",
    "\n",
    "_PDFMinerLoader_:  \n",
    "This extracts the raw text that is saved in the pdf document if it is available. This may not always be the case if for some reason the document was preprocessed as an image. The latter is very unlikely for financial documents\n",
    "\n",
    "_UnstructuredPDFLoader_:  \n",
    "In case the text is not available, this uses Optical Character Recognition (OCR) to extract text from an image.\n",
    "\n",
    "The idea is to let both run over a document and then compare to assure all necessary data is available for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f29b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary modules\n",
    "from langchain.document_loaders import (\n",
    "    PDFMinerLoader,\n",
    "    UnstructuredImageLoader,\n",
    "    UnstructuredPDFLoader\n",
    ")\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PDFMinerLoader(path)\n",
    "docs_text = pdf_loader.load()\n",
    "print(f\"Loaded {len(docs_text)} documents from {path}\")\n",
    "\n",
    "ocr_loader = UnstructuredPDFLoader(path)\n",
    "ocr_docs_text = ocr_loader.load()\n",
    "print(f\"Loaded {len(ocr_docs_text)} documents from {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423be741",
   "metadata": {},
   "source": [
    "### Structuring the text\n",
    "To efficiently make a RAG pipeline we want to split this data up into to _chunks_ so that when we add them to the LLM prompt, we only pass the important information and not waste tokens.\n",
    "\n",
    "A very basic implementation is the following with the _RecursiveCharacterTextSplitter_ also from the Langchain module.   \n",
    "This text splitter takes the input of string and splits it at logical boundaries such as \"\\n\\n\" (double newline) \"\\n\" (simple newline) in a hierarchal ranking of what could most likely be a semantical boundary\n",
    "\n",
    "Other options:\n",
    "- _TokenTextSplitter_ (splits by tokens not characters)\n",
    "- _NLTKTextSplitter_ or _SpacyTextSplitter_ (splits by sentences)\n",
    "- Domain specific splitter (e.g. Markdown / HTML)\n",
    "- Regex splitters (uses Regular Expressions to make the splitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9269608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551bf9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200,separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "texts = splitter.split_documents(docs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10 Chunks:\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a6587",
   "metadata": {},
   "source": [
    "### Creating embeddings and storing them\n",
    "\n",
    "Now we have the separate chunks of information that we may want to pass to a NN/LLM.   \n",
    "\n",
    "To assess which chunks we want to use for a prompt we need to have some metric top compare them top the question.   \n",
    "To do this we create _embeddings_, very high dimensional numerical vectors, of these chunks.   \n",
    "The euclidian distance between them is how close in meaning they are to each other.\n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "To create the embeddings we will use pretrained models such as _all-MiniLM-L6-v2_ that can be called using the _SentenceTransformerEmbeddings_ from Langchain module.   \n",
    "(You could also use OpenAI's Embedding model but this requires an API key)\n",
    "\n",
    "#### Vector DB\n",
    "\n",
    "To compare the diffrent chunks we will store the embeddings in a vector database. One of the most commonly used ones is the open sourced FAISS (Facebook AI Similarity Search). This can be called from the Langchain module as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingsmodel = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(texts, embeddingsmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e234e0a",
   "metadata": {},
   "source": [
    "### LLM\n",
    "\n",
    "Now that we have the RAG Database in place, all we need is the LLM. For this we will use open source models found on Huggingface such as Google's _flan-t5-base_.   \n",
    "The implementation pipeline is already built in the Langchain module as well.\n",
    "\n",
    "(Of course here you could also use OpenAIs pipeline as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aade59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca52034",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=512,temperature=0)\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9c8a9",
   "metadata": {},
   "source": [
    "### RAG Pipeline\n",
    "\n",
    "Now that we have the LLM and the Vector DB, we can bring it all together in a  simple RAG-pipeline.    \n",
    "In the Langchain module the _RetrievalQA_ class brings the LLM pipeline and the vector database together so that on each prompt that is passed it adds the chunks closest in meanings to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d84d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",                  # simplest “stuff” chain\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e7324",
   "metadata": {},
   "source": [
    "## QA:\n",
    "\n",
    "### Now that everything is in place, we can ask the network questions about the documents. Try it Out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the name of the company?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d7b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa.run(query)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
